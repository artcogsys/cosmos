% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}

\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\floatname{literal-block}{Listing }



\title{Cosmos Documentation}
\date{May 13, 2017}
\release{1.0}
\author{Marcel van Gerven}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


Contents:


\chapter{reinforcement\_learning package}
\label{reinforcement_learning:reinforcement-learning-package}\label{reinforcement_learning::doc}\label{reinforcement_learning:welcome-to-cosmos-s-documentation}

\section{Submodules}
\label{reinforcement_learning:submodules}

\section{reinforcement\_learning.agents module}
\label{reinforcement_learning:reinforcement-learning-agents-module}\label{reinforcement_learning:module-reinforcement_learning.agents}\index{reinforcement\_learning.agents (module)}\index{AACAgent (class in reinforcement\_learning.agents)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.AACAgent}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.agents.}\bfcode{AACAgent}}{\emph{model}, \emph{optimizer=None}, \emph{gamma=0.99}, \emph{beta=0.01}, \emph{cutoff=None}}{}
Bases: {\hyperref[reinforcement_learning:reinforcement_learning.agents.REINFORCEAgent]{\emph{\code{reinforcement\_learning.agents.REINFORCEAgent}}}}

Implements Advantage Actor-Critic algorithm
\index{train() (reinforcement\_learning.agents.AACAgent method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.AACAgent.train}\pysiglinewithargsret{\bfcode{train}}{\emph{observation}, \emph{reward}, \emph{done}}{}
Trains agent on cumulated reward (return)
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
action (Variable)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{NESAgent (class in reinforcement\_learning.agents)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.NESAgent}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.agents.}\bfcode{NESAgent}}{\emph{model}, \emph{nsteps=100}, \emph{npop=50}, \emph{sigma=0.1}, \emph{alpha=0.001}}{}
Bases: \code{object}

Implements Natural Evolution Strategies algorithm

\href{https://blog.openai.com/evolution-strategies/}{https://blog.openai.com/evolution-strategies/}
\href{https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d}{https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d}

hard to implement in same framework since the agent needs to run multiple versions of the task
\index{train() (reinforcement\_learning.agents.NESAgent method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.NESAgent.train}\pysiglinewithargsret{\bfcode{train}}{\emph{observation}, \emph{reward}, \emph{done}}{}
Trains agent on cumulated reward (return)
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
action (Variable)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{REINFORCEAgent (class in reinforcement\_learning.agents)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.REINFORCEAgent}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.agents.}\bfcode{REINFORCEAgent}}{\emph{model}, \emph{optimizer=None}, \emph{gamma=0.99}, \emph{beta=0.01}, \emph{cutoff=None}}{}
Bases: \code{object}

Implements REINFORCE algorithm
\index{entropy() (reinforcement\_learning.agents.REINFORCEAgent method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.REINFORCEAgent.entropy}\pysiglinewithargsret{\bfcode{entropy}}{\emph{pi}}{}
Computes entropy of policy
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{policy}} -- 

\end{description}\end{quote}

\end{fulllineitems}

\index{reset\_state() (reinforcement\_learning.agents.REINFORCEAgent method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.REINFORCEAgent.reset_state}\pysiglinewithargsret{\bfcode{reset\_state}}{}{}
Resets persistent states

\end{fulllineitems}

\index{score\_function() (reinforcement\_learning.agents.REINFORCEAgent method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.REINFORCEAgent.score_function}\pysiglinewithargsret{\bfcode{score\_function}}{\emph{action}, \emph{policy}}{}
Computes score
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{action}} (\emph{int}) -- 

\item {} 
\textbf{\texttt{policy}} -- 

\end{itemize}

\item[{Returns}] \leavevmode
score

\end{description}\end{quote}

\end{fulllineitems}

\index{test() (reinforcement\_learning.agents.REINFORCEAgent method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.REINFORCEAgent.test}\pysiglinewithargsret{\bfcode{test}}{\emph{observation}, \emph{reward}, \emph{done}}{}
Tests agent
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
action (Variable)

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (reinforcement\_learning.agents.REINFORCEAgent method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.agents.REINFORCEAgent.train}\pysiglinewithargsret{\bfcode{train}}{\emph{observation}, \emph{reward}, \emph{done}}{}
Trains agent on cumulated reward (return)
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
action (Variable)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{reinforcement\_learning.models module}
\label{reinforcement_learning:reinforcement-learning-models-module}\label{reinforcement_learning:module-reinforcement_learning.models}\index{reinforcement\_learning.models (module)}\index{ActorCriticModel (class in reinforcement\_learning.models)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.ActorCriticModel}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.models.}\bfcode{ActorCriticModel}}{\emph{net}, \emph{gpu=-1}}{}
Bases: {\hyperref[reinforcement_learning:reinforcement_learning.models.Model]{\emph{\code{reinforcement\_learning.models.Model}}}}

An actor model computes the action and policy from a predictor
\index{\_\_call\_\_() (reinforcement\_learning.models.ActorCriticModel method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.ActorCriticModel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{data}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{data}} -- observation

\item[{Returns}] \leavevmode
action and policy

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ActorModel (class in reinforcement\_learning.models)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.ActorModel}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.models.}\bfcode{ActorModel}}{\emph{net}, \emph{gpu=-1}}{}
Bases: {\hyperref[reinforcement_learning:reinforcement_learning.models.Model]{\emph{\code{reinforcement\_learning.models.Model}}}}

An actor model computes the action and policy from a predictor
\index{\_\_call\_\_() (reinforcement\_learning.models.ActorModel method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.ActorModel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{data}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{data}} -- observation

\item[{Returns}] \leavevmode
action and policy

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Model (class in reinforcement\_learning.models)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.Model}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.models.}\bfcode{Model}}{\emph{net}, \emph{gpu=-1}}{}
Bases: \code{chainer.link.Chain}

Model which wraps a network to generate predictions and compute policies
\index{has\_state (reinforcement\_learning.models.Model attribute)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.Model.has_state}\pysigline{\bfcode{has\_state}}
Checks if a network has persistent states
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
bool

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (reinforcement\_learning.models.Model method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.Model.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{data}}{}
Returns an action

\end{fulllineitems}

\index{reset\_state() (reinforcement\_learning.models.Model method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.models.Model.reset_state}\pysiglinewithargsret{\bfcode{reset\_state}}{}{}
\end{fulllineitems}


\end{fulllineitems}



\section{reinforcement\_learning.networks module}
\label{reinforcement_learning:reinforcement-learning-networks-module}\label{reinforcement_learning:module-reinforcement_learning.networks}\index{reinforcement\_learning.networks (module)}\index{MLP (class in reinforcement\_learning.networks)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.networks.MLP}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.networks.}\bfcode{MLP}}{\emph{n\_input=None}, \emph{n\_output=1}, \emph{n\_hidden=10}}{}
Bases: \code{chainer.link.Chain}

Multilayer perceptron
\index{has\_state (reinforcement\_learning.networks.MLP attribute)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.networks.MLP.has_state}\pysigline{\bfcode{has\_state}}
Checks if a network has persistent states
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
bool

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{RNN (class in reinforcement\_learning.networks)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.networks.RNN}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.networks.}\bfcode{RNN}}{\emph{n\_input=None}, \emph{n\_output=1}, \emph{n\_hidden=10}}{}
Bases: \code{chainer.link.Chain}
\index{has\_state (reinforcement\_learning.networks.RNN attribute)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.networks.RNN.has_state}\pysigline{\bfcode{has\_state}}
Checks if a network has persistent states
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
bool

\end{description}\end{quote}

\end{fulllineitems}

\index{reset\_state() (reinforcement\_learning.networks.RNN method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.networks.RNN.reset_state}\pysiglinewithargsret{\bfcode{reset\_state}}{}{}
Resets persistent states

\end{fulllineitems}


\end{fulllineitems}



\section{reinforcement\_learning.tasks module}
\label{reinforcement_learning:module-reinforcement_learning.tasks}\label{reinforcement_learning:reinforcement-learning-tasks-module}\index{reinforcement\_learning.tasks (module)}\index{EvidenceTask (class in reinforcement\_learning.tasks)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.tasks.EvidenceTask}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.tasks.}\bfcode{EvidenceTask}}{\emph{n=2}, \emph{p=0.8}}{}
Bases: \code{object}

Very simple task which only requires evaluating present evidence and does not require evidence integration.
The actor gets a reward when it correctly decides on the ground truth. Ground truth 0/1 determines probabilistically
the number of 0s or 1s as observations
\index{reset() (reinforcement\_learning.tasks.EvidenceTask method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.tasks.EvidenceTask.reset}\pysiglinewithargsret{\bfcode{reset}}{}{}
Resets state and generates new observations
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
observations, reward, done

\end{description}\end{quote}

\end{fulllineitems}

\index{step() (reinforcement\_learning.tasks.EvidenceTask method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.tasks.EvidenceTask.step}\pysiglinewithargsret{\bfcode{step}}{\emph{action}}{}
This task always produces a new state and observation after each decision
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{action}} -- agent(s) action

\item[{Returns}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{reinforcement\_learning.unit\_test module}
\label{reinforcement_learning:reinforcement-learning-unit-test-module}\label{reinforcement_learning:module-reinforcement_learning.unit_test}\index{reinforcement\_learning.unit\_test (module)}\index{UnitTest (class in reinforcement\_learning.unit\_test)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.unit_test.UnitTest}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.unit\_test.}\bfcode{UnitTest}}{\emph{methodName='runTest'}}{}
Bases: \code{unittest.case.TestCase}
\index{test\_aac\_stateful() (reinforcement\_learning.unit\_test.UnitTest method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.unit_test.UnitTest.test_aac_stateful}\pysiglinewithargsret{\bfcode{test\_aac\_stateful}}{}{}
Test Advantage Actor-Critic on stateful network

\end{fulllineitems}

\index{test\_reinforce\_stateful() (reinforcement\_learning.unit\_test.UnitTest method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.unit_test.UnitTest.test_reinforce_stateful}\pysiglinewithargsret{\bfcode{test\_reinforce\_stateful}}{}{}
Test REINFORCE on stateful network

\end{fulllineitems}

\index{test\_reinforce\_stateless() (reinforcement\_learning.unit\_test.UnitTest method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.unit_test.UnitTest.test_reinforce_stateless}\pysiglinewithargsret{\bfcode{test\_reinforce\_stateless}}{}{}
Test REINFORCE on stateless network

\end{fulllineitems}


\end{fulllineitems}



\section{reinforcement\_learning.world module}
\label{reinforcement_learning:module-reinforcement_learning.world}\label{reinforcement_learning:reinforcement-learning-world-module}\index{reinforcement\_learning.world (module)}\index{World (class in reinforcement\_learning.world)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.world.World}\pysiglinewithargsret{\strong{class }\code{reinforcement\_learning.world.}\bfcode{World}}{\emph{agents}, \emph{out='result'}}{}
Bases: \code{object}

Wrapper object which takes care of training and testing on some data iterator for one or more agents
\index{test() (reinforcement\_learning.world.World method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.world.World.test}\pysiglinewithargsret{\bfcode{test}}{\emph{task}, \emph{n\_steps}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{task}} -- task to run agent(s) on

\item {} 
\textbf{\texttt{n\_steps}} (\emph{int}) -- number of steps to train on

\end{itemize}

\item[{Returns}] \leavevmode
test loss and reward

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (reinforcement\_learning.world.World method)}

\begin{fulllineitems}
\phantomsection\label{reinforcement_learning:reinforcement_learning.world.World.train}\pysiglinewithargsret{\bfcode{train}}{\emph{task}, \emph{n\_steps}, \emph{snapshot=0}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{task}} -- task to run agent(s) on

\item {} 
\textbf{\texttt{n\_steps}} (\emph{int}) -- number of steps to train on

\item {} 
\textbf{\texttt{snapshot}} (\emph{int}) -- whether or not to save model after each epochs modulo snapshot

\end{itemize}

\item[{Returns}] \leavevmode
rewards

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Module contents}
\label{reinforcement_learning:module-reinforcement_learning}\label{reinforcement_learning:module-contents}\index{reinforcement\_learning (module)}

\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUspan{xref,std,std-ref}{genindex}

\item {} 
\DUspan{xref,std,std-ref}{modindex}

\item {} 
\DUspan{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{r}
\item {\texttt{reinforcement\_learning}}, \pageref{reinforcement_learning:module-reinforcement_learning}
\item {\texttt{reinforcement\_learning.agents}}, \pageref{reinforcement_learning:module-reinforcement_learning.agents}
\item {\texttt{reinforcement\_learning.models}}, \pageref{reinforcement_learning:module-reinforcement_learning.models}
\item {\texttt{reinforcement\_learning.networks}}, \pageref{reinforcement_learning:module-reinforcement_learning.networks}
\item {\texttt{reinforcement\_learning.tasks}}, \pageref{reinforcement_learning:module-reinforcement_learning.tasks}
\item {\texttt{reinforcement\_learning.unit\_test}}, \pageref{reinforcement_learning:module-reinforcement_learning.unit_test}
\item {\texttt{reinforcement\_learning.world}}, \pageref{reinforcement_learning:module-reinforcement_learning.world}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
